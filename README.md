# DeepCFR_transformer
Algorithm Deep CFR with Transformers architecture. Train on Poker - texasholdem 0.11.0(https://github.com/SirRender00/texasholdem) on Python &amp; Pytorch

# RU
Использованные паттерны для сходимости представленного алгоритма
1. Пространство действий:
Большое пространство действий вызванное разбросом ActionType: Raise из-за безлимитного разброса ,своей масштабностью, сводит код к ограничению пространства действий динамическими Raise от стадии игры. Но даже подобное ограничение приводит к очень долгому ветвлению действий внутри одной раздачи -> это приводит к необходимости ограничения действий и объема subgame в рекурсии, которую отыгрывает DeepCFR, что приводит нас к идеи MCCFR где используется не полная разведка но присутствует доказательство сходимости такого алгоритма, при использовании IS-весов подигр.

2. Ценность действий для модели при не полной разведке:
Из-за политике отбора только части действий из большого пространства доступных мы считаем: для основных u_exact(u_mean по каждому подузлу), для действий не попавших в множество выбранных u_exact = 0, что ни как не влияет на модель, а теория MC-CFR при большом объеме обучения предполагает что каждое действие так или иначе попадет в выборку.

3. Стратификация выборок внутри буфера по фазам.

4. Использование функций потерь из DeepCFR: для Adv: MSE, для Policy: KL-дивергенция. Loss усредняется с весами IS_weight. (как в пункте 1 по статье сходимости MCCFR)

5. Архитектура модели и texasholdem:
5.1. LayerNorm,Dropout - внутри DeepCFR
5.2. MonyeNorm - используем в модели bb = 1, buyin = 200, как простую альтернативу для нормализации
5.3. MaskedSoftmax - для сокрытия нерелевантных действий.
5.4. Type\Pos-embedding - ak.BERT архитектура для уточнения разницы между игроками, картами игрока и стола.

# Особенности архитектуры выбранной игры и подводные камни в ней.
1. Traverse не через deepcopy. 
Из-за внутренней особенности texasholdem, мы не можем копировать игру и отыгрывать её subgames с каждого узла - раздача не фиксируется. Единственный способ фиксировать раздачу -> заморозить np.random_seed(n), на котором строится случайность и отыгрывать новую раздачу из раза в раз по истории и этому seed, используя "случайность внутри случайности" для выбора случайных действия. 


# Будущее
1. Данный код рассчитан на No limit TexasHoldem, но полноценное тестирование алгоритма на игре с настолько огромном пространстве потенциальных состояний выглядит ресурсоёмким и не укладывается во временные рамки которые можно просто так отвести под неё. Для будущих тестов будет сделать ещё один репозиторий с тестами на более простом Kuhn Poker, или чуть большей игре с нулевой суммой.

2. Worker_process для параллельного наигрыша игры через python api - texasholdem. worker_process Функция и parallel_deep_cfr_trainer. Multiprocessing. Ввиду слишком большого пространства необходимых данных в буфере для обучения модели. 
